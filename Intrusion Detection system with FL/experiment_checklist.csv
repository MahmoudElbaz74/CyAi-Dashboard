Area,Experiment,Params/Variants,Compare Against,Metrics
Model architectures,Baseline Hybrid CNN–LSTM (paper replication),"lr=0.001, batch=64, local_epochs=5",Paper reported Hybrid CNN–LSTM,"Accuracy, F1, ROC-AUC, per-class F1"
Model architectures,Transformer encoder as local model,"n_layers=2, heads=4, d_model=128",Hybrid CNN–LSTM,"Accuracy, F1, convergence speed"
Model architectures,Tabular Transformer (categorical + numeric embeddings),"embed_dim=128, n_layers=2",Hybrid CNN–LSTM,"Accuracy, F1, ROC-AUC"
Model architectures,CNN → Transformer hybrid,"CNN(kernel=3, filters=64) + Transformer encoder",CNN–LSTM,"Accuracy, F1, convergence"
Privacy/Compression,DP-SGD with varying ε,"ε={0.5,1.5,3.0}, noise_multiplier tuned",ε=1.5 (paper),"Accuracy, F1, privacy loss"
Privacy/Compression,Gradient clipping threshold sweep,"clip_norm={0.5,1.0,2.0}",clip_norm=1.0,"Accuracy, convergence stability"
Privacy/Compression,Pruning sparsity sweep,"sparsity={0%,50%,80%}",No pruning,"Accuracy, comm cost reduction"
Robustness/Aggregation,"Compare FedAvg, TrimmedMean, Median, Krum","Malicious clients={5%,10%}, attacks={label-flip, backdoor}",FedAvg baseline,"Accuracy under attack, F1, robustness"
Robustness/Aggregation,Add anomaly detection on client updates,Cosine similarity + reputation weighting,No defense,"Attack detection rate, accuracy retention"
Personalization,FedPer (personal layers),"Freeze shared base, fine-tune heads",FedAvg global,"Avg client F1, worst-client F1, variance"
Personalization,Local fine-tuning after global round,1–3 local epochs,No fine-tune,Per-client performance distribution
Explainability,Federated SHAP aggregation,Local SHAP → aggregate attribution vectors,Local-only explanations,"Attribution stability, anomaly detection"
Graph models,GraphSAGE / GAT on host graphs,"Window=30s, 2-layer GNN",Flow-based models,Host-level detection accuracy
